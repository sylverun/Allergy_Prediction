{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882a40d-4485-4f74-8cf2-f683bb79fcfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2acd42a-6cff-4570-b3ff-6d048fff19da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Pytorch on CSV_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "775c8cb0-69ee-4517-aad5-a7e2655d5986",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: session_info in /opt/conda/lib/python3.9/site-packages (1.0.0)\n",
      "Requirement already satisfied: stdlib-list in /opt/conda/lib/python3.9/site-packages (from session_info) (0.8.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.9/site-packages (0.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install session_info\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d342d69-5da6-4959-b539-d84b5e328343",
   "metadata": {},
   "source": [
    "#### Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5050e33-94ce-4eca-9393-850d391d2740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "imblearn            0.10.1\n",
       "numpy               1.21.5\n",
       "pandas              1.4.1\n",
       "session_info        1.0.0\n",
       "sklearn             1.0.2\n",
       "torch               1.11.0+cu102\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "asttokens                   NA\n",
       "astunparse                  1.6.3\n",
       "backcall                    0.2.0\n",
       "beta_ufunc                  NA\n",
       "binom_ufunc                 NA\n",
       "bottleneck                  1.3.4\n",
       "colorama                    0.4.4\n",
       "cython_runtime              NA\n",
       "dateutil                    2.8.2\n",
       "debugpy                     1.5.1\n",
       "decorator                   5.1.1\n",
       "dill                        0.3.4\n",
       "entrypoints                 0.4\n",
       "executing                   0.8.3\n",
       "google                      NA\n",
       "hypergeom_ufunc             NA\n",
       "importlib_metadata          NA\n",
       "ipykernel                   6.9.2\n",
       "ipython_genutils            0.2.0\n",
       "ipywidgets                  7.7.0\n",
       "jedi                        0.18.1\n",
       "joblib                      1.2.0\n",
       "jupyter_server              1.15.6\n",
       "mpl_toolkits                NA\n",
       "nbinom_ufunc                NA\n",
       "numexpr                     2.8.0\n",
       "packaging                   21.3\n",
       "parso                       0.8.3\n",
       "pexpect                     4.8.0\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "prompt_toolkit              3.0.27\n",
       "psutil                      5.9.0\n",
       "ptyprocess                  0.7.0\n",
       "pure_eval                   0.2.2\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.6.0\n",
       "pydevd_concurrency_analyser NA\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pygments                    2.11.2\n",
       "pytz                        2021.3\n",
       "ruamel                      NA\n",
       "scipy                       1.8.0\n",
       "setuptools                  60.9.3\n",
       "six                         1.16.0\n",
       "stack_data                  0.2.0\n",
       "threadpoolctl               3.1.0\n",
       "tornado                     6.1\n",
       "tqdm                        4.62.3\n",
       "traitlets                   5.1.1\n",
       "typing_extensions           NA\n",
       "wcwidth                     0.2.5\n",
       "zipp                        NA\n",
       "zmq                         22.3.0\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             8.1.1\n",
       "jupyter_client      7.1.2\n",
       "jupyter_core        4.9.2\n",
       "jupyterlab          3.3.2\n",
       "notebook            6.4.10\n",
       "-----\n",
       "Python 3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:24:11) [GCC 9.4.0]\n",
       "Linux-5.4.0-1104-azure-x86_64-with-glibc2.31\n",
       "-----\n",
       "Session information updated at 2023-06-04 14:11\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset , random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import imblearn.over_sampling as oversampling\n",
    "import imblearn.under_sampling as undersampling\n",
    "import imblearn.combine as combination\n",
    "import random\n",
    "\n",
    "from torch.utils.data import TensorDataset # to recreate the modified dataset at each epoch\n",
    "\n",
    "import session_info\n",
    "\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337bddde-2a7a-4b3b-96bc-bb21e0c29db5",
   "metadata": {},
   "source": [
    "#### Set the random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d2ae21-1a9b-4a2c-84c7-ac0295c6b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 64\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49379e92-05d3-4239-a3d3-712997cc19b7",
   "metadata": {},
   "source": [
    "#### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6a64788-604f-49be-b650-21397ff0c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data= pd.read_csv('data/train.csv', low_memory=False)\n",
    "data_test= pd.read_csv('data/test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae52296-8669-4aee-a351-0528a97fcd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2989 entries, 0 to 2988\n",
      "Columns: 365 entries, Patient_ID to Type_of_Venom_Allergy_IGE_Venom\n",
      "dtypes: float64(322), int64(32), object(11)\n",
      "memory usage: 8.3+ MB\n"
     ]
    }
   ],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be2bf4-8a56-4b4f-a4c4-88a1b08b7d2b",
   "metadata": {},
   "source": [
    "#### Looking which are the targets to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb6fa6c-105e-425b-8c32-1f57f7f99e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Type_of_Food_Allergy_Other_Legumes', 'Type_of_Food_Allergy_Tree_Nuts', 'Type_of_Food_Allergy_Other', 'Type_of_Respiratory_Allergy_CONJ', 'Venom_Allergy', 'Food_Allergy', 'Type_of_Food_Allergy_Fruits_and_Vegetables', 'Respiratory_Allergy', 'Type_of_Respiratory_Allergy_IGE_Dander_Animals', 'Type_of_Respiratory_Allergy_IGE_Mite_Cockroach', 'Type_of_Respiratory_Allergy_IGE_Pollen_Herb', 'Type_of_Food_Allergy_Peanut', 'trustii_id', 'Type_of_Venom_Allergy_ATCD_Venom', 'Type_of_Respiratory_Allergy_ARIA', 'Severe_Allergy', 'Allergy_Present', 'Type_of_Venom_Allergy_IGE_Venom', 'Type_of_Food_Allergy_Aromatics', 'Type_of_Food_Allergy_Mammalian_Milk', 'Type_of_Food_Allergy_TPO', 'Type_of_Respiratory_Allergy_IGE_Pollen_Tree', 'Type_of_Food_Allergy_Oral_Syndrom', 'Type_of_Food_Allergy_Cereals_&_Seeds', 'Type_of_Respiratory_Allergy_IGE_Molds_Yeast', 'Type_of_Food_Allergy_Shellfish', 'Type_of_Respiratory_Allergy_IGE_Pollen_Gram', 'Type_of_Respiratory_Allergy_GINA', 'Type_of_Food_Allergy_Fish', 'Type_of_Food_Allergy_Egg'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_cols = set(raw_data.columns) ^ set(data_test.columns)\n",
    "print(missing_cols)\n",
    "len(missing_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f04a67b-5229-43a9-a78b-508d6a98aae3",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9ebd1-0769-4fc1-9573-d59001ea9b25",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing for the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e6fcd1-b54f-490c-b840-d5f99e86cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_of_Targets =['Allergy_Present', 'Severe_Allergy', 'Respiratory_Allergy', 'Food_Allergy', 'Venom_Allergy',\n",
    "                     'Type_of_Respiratory_Allergy_ARIA', 'Type_of_Respiratory_Allergy_CONJ', \n",
    "                     'Type_of_Respiratory_Allergy_GINA', 'Type_of_Respiratory_Allergy_IGE_Pollen_Gram',\n",
    "                     'Type_of_Respiratory_Allergy_IGE_Pollen_Herb', 'Type_of_Respiratory_Allergy_IGE_Pollen_Tree',\n",
    "                     'Type_of_Respiratory_Allergy_IGE_Dander_Animals', 'Type_of_Respiratory_Allergy_IGE_Mite_Cockroach',\n",
    "                     'Type_of_Respiratory_Allergy_IGE_Molds_Yeast', 'Type_of_Food_Allergy_Aromatics', 'Type_of_Food_Allergy_Other',\n",
    "                     'Type_of_Food_Allergy_Cereals_&_Seeds', 'Type_of_Food_Allergy_Egg', 'Type_of_Food_Allergy_Fish',\n",
    "                     'Type_of_Food_Allergy_Fruits_and_Vegetables', 'Type_of_Food_Allergy_Mammalian_Milk', \n",
    "                     'Type_of_Food_Allergy_Oral_Syndrom', 'Type_of_Food_Allergy_Other_Legumes', 'Type_of_Food_Allergy_Peanut',\n",
    "                     'Type_of_Food_Allergy_Shellfish', 'Type_of_Food_Allergy_TPO', 'Type_of_Food_Allergy_Tree_Nuts',\n",
    "                     'Type_of_Venom_Allergy_ATCD_Venom', 'Type_of_Venom_Allergy_IGE_Venom']\n",
    "def preprocessing_data(df):\n",
    "    df = df.drop('Food_Type_0', axis =1)\n",
    "    df.replace(-1, 0, inplace=True)\n",
    "    data_noNAN = df.fillna(-1)\n",
    "    # obtain Targets\n",
    "    Targets = data_noNAN.loc[:,liste_of_Targets]\n",
    "    # filter feautures\n",
    "    X1=data_noNAN.loc[:, ['Chip_Type','Age','Gender','French_Residence_Department','Blood_Month_sample']]\n",
    "    X= data_noNAN.iloc[:, 8:-29]\n",
    "    data = pd.concat( [X1, X] , axis=1)\n",
    "    # handle the 'Treatment_of_rhinitis' feature\n",
    "    data['Treatment_of_rhinitis'] = data['Treatment_of_rhinitis'].astype(str)\n",
    "    data['Treatment_of_rhinitis'] = data['Treatment_of_rhinitis'].str.replace('.0', '', regex=True)\n",
    "   \n",
    "    # Preprocessing of numerical data\n",
    "    \n",
    "    ## Transform continuous values of chips features into discrete values\n",
    "    \n",
    "    float_cols= data.select_dtypes(include=['float64'])\n",
    "    filtered_cols = float_cols.columns[float_cols.apply(lambda x: (x % 1 != 0).any())] # it doesn't matter if we miss the columns fill with 0 as at the end, 0 keep their values\n",
    "    filtered_data = data[filtered_cols]\n",
    "    \n",
    "    ### Create a boolean mask for non -1 values\n",
    "    mask = filtered_data != -1\n",
    "    \n",
    "    ### Apply Min-Max scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(filtered_data[mask])\n",
    "    \n",
    "    ### Convert the scaled array back to a DataFrame and apply 'discretisation' \n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=filtered_data.columns)\n",
    "\n",
    "    thresholds = [0, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 2.5e-2, 5e-2, 1e-1, 5e-1, 1]\n",
    "\n",
    "    scaled_df[(scaled_df > thresholds[0]) & (scaled_df < thresholds[1])] = 1.5\n",
    "    scaled_df[(scaled_df >= thresholds[1]) & (scaled_df < thresholds[2])] = 2\n",
    "    scaled_df[(scaled_df >= thresholds[2]) & (scaled_df < thresholds[3])] = 3\n",
    "    scaled_df[(scaled_df >= thresholds[3]) & (scaled_df < thresholds[4])] = 4\n",
    "    scaled_df[(scaled_df >= thresholds[4]) & (scaled_df < thresholds[5])] = 5\n",
    "    scaled_df[(scaled_df >= thresholds[5]) & (scaled_df < thresholds[6])] = 6\n",
    "    scaled_df[(scaled_df >= thresholds[6]) & (scaled_df < thresholds[7])] = 7\n",
    "    scaled_df[(scaled_df >= thresholds[7]) & (scaled_df < thresholds[8])] = 8\n",
    "    scaled_df[(scaled_df >= thresholds[8]) & (scaled_df < thresholds[9])] = 9\n",
    "    scaled_df[(scaled_df >= thresholds[9]) & (scaled_df < (thresholds[10]+0.01))] = 10\n",
    "\n",
    "    ### Replace NaN values with -1\n",
    "    scaled_df = (scaled_df/10).fillna(-1)\n",
    "    \n",
    "    ### Replace columns in the dataframe of features\n",
    "    columns_to_update = scaled_df.columns\n",
    "    data[columns_to_update] = scaled_df[columns_to_update]\n",
    "    \n",
    "    ##  Get_dummies of the 'object' type columns\n",
    "    \n",
    "    columns_to_encode = ['Chip_Type', 'French_Residence_Department', 'French_Region',\n",
    "         'Treatment_of_athsma', 'Age_of_onsets',\n",
    "       'General_cofactors', 'Treatment_of_atopic_dematitis','Treatment_of_rhinitis']\n",
    "    \n",
    "    ### Split the columns using multiple delimiters and create dummy columns\n",
    "    dummy_dfs = []\n",
    "    for col in columns_to_encode:\n",
    "        # Split the data in the column that use  delimiters\n",
    "        data[col] = data[col].astype(str)\n",
    "        data[col] = data[col].apply(lambda x: [i.strip() for i in re.split('[,.]', x)])\n",
    "\n",
    "        # Create dummy columns\n",
    "        dummy_df = pd.get_dummies(data[col].apply(pd.Series).stack(), prefix=f\"{col}\", prefix_sep='_').groupby(level=0).sum()\n",
    "        dummy_dfs.append(dummy_df)\n",
    "\n",
    "    ### Concatenate the original DataFrame with the dummy columns\n",
    "    df_final = pd.concat([data] + dummy_dfs, axis=1)\n",
    "\n",
    "    ### Drop the original columns from the final dataset\n",
    "    df_final.drop(columns=columns_to_encode, inplace=True)\n",
    "    \n",
    "    # Converting all values into 'float16' type\n",
    "    encode_data = df_final.astype('float16')\n",
    "    print(encode_data.info())\n",
    "    \n",
    "    return encode_data,Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "360d7a49-4c17-4514-8c22-c6683e7a9d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2989 entries, 0 to 2988\n",
      "Columns: 467 entries, Age to Treatment_of_rhinitis_9\n",
      "dtypes: float16(467)\n",
      "memory usage: 2.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "encode_data,Targets = preprocessing_data(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea79fbd-4c63-4e10-97ff-81ae06b3e04b",
   "metadata": {},
   "source": [
    "### Preprocessing for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f1a3d9-1c1b-4f56-8300-53d8096a9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data_test(df):\n",
    "    df = df.drop('Food_Type_0', axis =1)\n",
    "    df.replace(-1, 0, inplace=True)\n",
    "    data_test_noNAN = df.fillna(-1)\n",
    "    # filter feautures\n",
    "    X1=data_test_noNAN.loc[:, ['Chip_Type']]\n",
    "    X= data_test_noNAN.iloc[:, 5:]\n",
    "    data = pd.concat( [X1, X] , axis=1)\n",
    "    # handle the 'Treatment_of_rhinitis' feature\n",
    "    data['Treatment_of_rhinitis'] = data['Treatment_of_rhinitis'].astype(str)\n",
    "    data['Treatment_of_rhinitis'] = data['Treatment_of_rhinitis'].str.replace('.0', '', regex=True)\n",
    "    # handle 'Age_of_onsets' which don't have the same format in data test and train\n",
    "    data['Age_of_onsets'] = data['Age_of_onsets'].astype(str)\n",
    "\n",
    "    # Preprocessing of numerical data\n",
    "\n",
    "    ## Transform continuous values of chips features into discrete values\n",
    "\n",
    "    float_cols= data.select_dtypes(include=['float64'])\n",
    "    filtered_cols = float_cols.columns[float_cols.apply(lambda x: (x % 1 != 0).any())] # it doesn't matter if we miss the columns fill with 0 as at the end, 0 keep their values\n",
    "    filtered_data = data[filtered_cols]\n",
    "\n",
    "    ### Create a boolean mask for non -1 values\n",
    "    mask = filtered_data != -1\n",
    "\n",
    "    ### Apply Min-Max scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(filtered_data[mask])\n",
    "\n",
    "    ### Convert the scaled array back to a DataFrame and apply 'discretisation' \n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=filtered_data.columns)\n",
    "\n",
    "    thresholds = [0, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 2.5e-2, 5e-2, 1e-1, 5e-1, 1]\n",
    "\n",
    "    scaled_df[(scaled_df > thresholds[0]) & (scaled_df < thresholds[1])] = 1.5\n",
    "    scaled_df[(scaled_df >= thresholds[1]) & (scaled_df < thresholds[2])] = 2\n",
    "    scaled_df[(scaled_df >= thresholds[2]) & (scaled_df < thresholds[3])] = 3\n",
    "    scaled_df[(scaled_df >= thresholds[3]) & (scaled_df < thresholds[4])] = 4\n",
    "    scaled_df[(scaled_df >= thresholds[4]) & (scaled_df < thresholds[5])] = 5\n",
    "    scaled_df[(scaled_df >= thresholds[5]) & (scaled_df < thresholds[6])] = 6\n",
    "    scaled_df[(scaled_df >= thresholds[6]) & (scaled_df < thresholds[7])] = 7\n",
    "    scaled_df[(scaled_df >= thresholds[7]) & (scaled_df < thresholds[8])] = 8\n",
    "    scaled_df[(scaled_df >= thresholds[8]) & (scaled_df < thresholds[9])] = 9\n",
    "    scaled_df[(scaled_df >= thresholds[9]) & (scaled_df < (thresholds[10]+0.01))] = 10\n",
    "\n",
    "    ### Replace NaN values with -1\n",
    "    scaled_df = (scaled_df/10).fillna(-1)\n",
    "\n",
    "    ### Replace columns in the dataframe of features\n",
    "    columns_to_update = scaled_df.columns\n",
    "    data[columns_to_update] = scaled_df[columns_to_update]\n",
    "    \n",
    "    ##  Get_dummies of the 'object' type columns\n",
    "    \n",
    "    columns_to_encode = ['Chip_Type', 'French_Residence_Department', 'French_Region',\n",
    "         'Treatment_of_athsma', 'Age_of_onsets',\n",
    "       'General_cofactors', 'Treatment_of_atopic_dematitis','Treatment_of_rhinitis']\n",
    "    \n",
    "    ### Split the columns using multiple delimiters and create dummy columns\n",
    "    dummy_dfs = []\n",
    "    for col in columns_to_encode:\n",
    "        # Split the data in the column that use  delimiters\n",
    "        data[col] = data[col].astype(str)\n",
    "        data[col] = data[col].apply(lambda x: [i.strip() for i in re.split('[,.]', x)])\n",
    "\n",
    "        # Create dummy columns\n",
    "        dummy_df = pd.get_dummies(data[col].apply(pd.Series).stack(), prefix=f\"{col}\", prefix_sep='_').groupby(level=0).sum()\n",
    "        dummy_dfs.append(dummy_df)\n",
    "\n",
    "    ### Concatenate the original DataFrame with the dummy columns\n",
    "    df_final = pd.concat([data] + dummy_dfs, axis=1)\n",
    "\n",
    "    ### Drop the original columns from the final dataset\n",
    "    df_final.drop(columns=columns_to_encode, inplace=True)\n",
    "    \n",
    "    # Converting all values into 'float16' type\n",
    "    encode_data = df_final.astype('float16')\n",
    "    print(encode_data.info())\n",
    "    \n",
    "    return encode_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e91d654d-cdfe-4a30-98c3-3ce314825760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1282 entries, 0 to 1281\n",
      "Columns: 448 entries, Age to Treatment_of_rhinitis_9\n",
      "dtypes: float16(448)\n",
      "memory usage: 1.1 MB\n",
      "None\n",
      "{'French_Region_regionN', 'French_Residence_Department_deptK', 'French_Residence_Department_deptZZZ', 'Treatment_of_atopic_dematitis_7', 'Treatment_of_athsma_8', 'French_Residence_Department_deptNNN', 'French_Residence_Department_deptP', 'French_Residence_Department_deptQQQ', 'French_Residence_Department_deptUU', 'French_Residence_Department_deptW', 'French_Residence_Department_deptAAAA', 'General_cofactors_11', 'French_Residence_Department_deptDD', 'French_Residence_Department_deptCCCC', 'French_Residence_Department_deptIII', 'French_Residence_Department_deptT', 'French_Residence_Department_deptOOO', 'French_Residence_Department_deptMMM', 'French_Residence_Department_deptDDD', 'French_Residence_Department_deptJJJ', 'French_Residence_Department_deptHHH', 'French_Residence_Department_deptRRR', 'French_Residence_Department_deptU'}\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1282 entries, 0 to 1281\n",
      "Columns: 467 entries, Age to Treatment_of_rhinitis_9\n",
      "dtypes: float16(467)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "encode_data_test = preprocessing_data_test(data_test)\n",
    "missing_cols = set(encode_data.columns) ^ set(encode_data_test.columns)\n",
    "print(missing_cols)\n",
    "len(missing_cols)\n",
    "encode_data_test = encode_data_test.reindex(columns=encode_data.columns, fill_value=0).astype('float16')\n",
    "encode_data_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9dd95-935e-47b5-8ea6-f5ef45d2c2de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## For one class (each class can use different hyperparameters to achieve the best results so we gonna explore all targets one by one and keep the best model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbd969-54d0-4990-8523-97d2d9bb0664",
   "metadata": {},
   "source": [
    "## For all targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afce995-909c-4471-a572-74329f523442",
   "metadata": {},
   "source": [
    "#### Transform 9 into 2 for multiclass pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c36447-0626-4789-afa9-42af739cc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "Targets.replace(9, 2, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46d775-5f6e-4e53-84f6-ecb9b64cffcf",
   "metadata": {},
   "source": [
    "#### General definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3955831-38cd-4ba3-98ba-89fa28de5fe6",
   "metadata": {},
   "source": [
    "##### The custom dataset that will be used to store datas as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45366904-b4fe-41ab-b9f4-8a67b2cb33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.features = self.data.iloc[:, :-1].values\n",
    "        self.labels = self.data.iloc[:, -1].values\n",
    "\n",
    "        if self.transform is not None:\n",
    "            self.features, self.labels = self.transform.fit_resample(self.features, self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    @property\n",
    "    def num_features(self):\n",
    "        return self.features.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return len(set(self.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1fc7e-ff8f-4ef3-bf8e-e71ba9747cb3",
   "metadata": {},
   "source": [
    "##### The class_weight calculator that will be used after each imblearn transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7496f2-04a0-4d97-ae5c-9583ed305886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(dataset):\n",
    "    # Convert labels to a PyTorch tensor\n",
    "    labels_tensor = torch.from_numpy(dataset.labels)\n",
    "    \n",
    "    # Get the number of samples in each class\n",
    "    class_counts = torch.bincount(labels_tensor)\n",
    "    \n",
    "    # Calculate the weight for each class as the inverse of its sample count\n",
    "    total_samples = torch.sum(class_counts)\n",
    "    class_weights = total_samples / (class_counts * len(class_counts))\n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb2a7f-a301-4a88-a456-fc2c6124b1f3",
   "metadata": {},
   "source": [
    "##### The dico of sampling technique that will be used to randomly chose the enhance data method at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b56a4d06-121b-448f-b662-97c8edcc29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_techniques = {\n",
    "    2: oversampling.RandomOverSampler(),\n",
    "    #1: oversampling.SMOTE(),\n",
    "    3: oversampling.BorderlineSMOTE(),\n",
    "    #0: oversampling.SVMSMOTE(),\n",
    "    #4: undersampling.TomekLinks(sampling_strategy='auto'),\n",
    "    4: undersampling.TomekLinks(sampling_strategy='all'),\n",
    "    5: combination.SMOTETomek(sampling_strategy='auto'),\n",
    "    6: combination.SMOTETomek(sampling_strategy='all'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ae3d8-d12b-4484-abd9-16b441d9a847",
   "metadata": {},
   "source": [
    "##### The architecture of the models to train and used for generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acbc4b30-d9b8-41db-8570-715e29a8c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Allergy_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,hidden_size, num_class,dropout_rate):\n",
    "\n",
    "        super(Allergy_Net,self).__init__()\n",
    "        self.linear1= nn.Linear(input_size,hidden_size)\n",
    "        self.linear2= nn.Linear(hidden_size,int(hidden_size/8))\n",
    "        self.linear3= nn.Linear(int(hidden_size/8),int(hidden_size/16))\n",
    "        self.linear4= nn.Linear(int(hidden_size/16),num_class)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(int(hidden_size/16))\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        x = torch.relu(self.linear1(inputs))\n",
    "        x= self.dropout1(x)\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x= self.dropout1(x)\n",
    "        x = torch.relu(self.linear3(x))\n",
    "        x= self.dropout1(x)\n",
    "        x= self.batchnorm1(x)\n",
    "        outputs= self.linear4(x)\n",
    "\n",
    "        # no softmax because Cross entropy Loss\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c831b-65f8-46e8-9e44-c12b415713b4",
   "metadata": {},
   "source": [
    "##### The training process to check how the model evolve during training and changing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "857bf058-53cc-4590-abe8-802077293c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_by_target(column,batch_size=512,learning_rate = 1e-3,dropout_rate= 0.6, weight_decay=1e-2, factor=0.75):\n",
    "    print(Targets[column].value_counts())\n",
    "    dataset_panda= pd.concat([encode_data,Targets[column]], axis = 1)\n",
    "    \n",
    "    # Establish the splitting and the Dataset/Loaders for the evaluation part\n",
    "    batch_size= batch_size\n",
    "    train_data, test_data = train_test_split(dataset_panda, test_size=0.25, random_state=123)\n",
    "    dataset_test=CustomDataset(test_data)\n",
    "    dataset_all=CustomDataset(dataset_panda)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "    test_all = DataLoader(dataset_all, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # define hyperparameters\n",
    "    input_size= dataset_all.num_features\n",
    "    hidden_size= 2048\n",
    "    num_class = dataset_all.num_classes\n",
    "    num_epochs=60\n",
    "    learning_rate = learning_rate\n",
    "    print(input_size, num_class)\n",
    "    \n",
    "    # Call a model and define loss and optimizer\n",
    "    model= Allergy_Net(input_size,hidden_size,num_class,dropout_rate).to(device)\n",
    "    criterion= nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=factor, patience=5, verbose=True)\n",
    "    \n",
    "    \n",
    "    # Initialization of some indicators that are used to save best model during training\n",
    "    best_f1_score = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Training loop\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        random_key = random.randint(2, 6)\n",
    "        selected_technique = sampling_techniques[random_key]\n",
    "        print(selected_technique)\n",
    "        # Apply the sampling technique\n",
    "        train_dataset = CustomDataset(train_data, transform=selected_technique)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "         # Update class weights\n",
    "        class_weights = calculate_class_weights(train_dataset)\n",
    "        criterion.weight = class_weights\n",
    "\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        model.train()\n",
    "        for i,(data,labels) in enumerate (dataloader):\n",
    "            data= data.to(device)\n",
    "            labels= labels.to(device)\n",
    "\n",
    "            #forward\n",
    "            outputs=model(data)\n",
    "            loss= criterion(outputs,labels)\n",
    "\n",
    "            #backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Calculate some metrics\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "        acc = accuracy_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "\n",
    "        print (f'epoch {epoch+1}/{num_epochs}, loss = {loss:.5f}, train_acc = {acc:.4f}, F1 Score_Train = {f1:.4f}')\n",
    "\n",
    "        # Test \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            true_labels_test = []\n",
    "            predicted_labels_test = []\n",
    "            for i,(data,labels) in enumerate (test_loader):\n",
    "                data= data.to(device)\n",
    "                labels= labels.to(device)\n",
    "\n",
    "                outputs = model(data)\n",
    "\n",
    "                # return value and index of the best class\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "                true_labels_test.extend(labels.cpu().numpy())\n",
    "                predicted_labels_test.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "            test_accuracy = accuracy_score(true_labels_test, predicted_labels_test)\n",
    "            f1_test = f1_score(true_labels_test, predicted_labels_test, average='macro')\n",
    "\n",
    "            lr_scheduler.step(f1_test + (f1*0.1))\n",
    "\n",
    "            # Check if the current model is the best one based on f1 score\n",
    "\n",
    "            if f1_test + (f1*0.5) > best_f1_score:\n",
    "                best_f1_score = f1_test+(f1*0.5) \n",
    "                best_model_state = model.state_dict()\n",
    "                torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "                print('\\033[91m'+'MODEL_SAVE')\n",
    "                print(f'Accuracy_test = {test_accuracy:.4f}, F1 Score_test = {f1_test:.4f}'+'\\033[0m')\n",
    "\n",
    "            print(' ')\n",
    "            \n",
    "    # eval_final_before saving        \n",
    "    model = Allergy_Net(input_size, hidden_size, num_class,dropout_rate).to(device)\n",
    "\n",
    "    # Load the saved model state dictionary\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Test       \n",
    "    with torch.no_grad():\n",
    "        true_labels_test = []\n",
    "        predicted_labels_test = []\n",
    "        for i,(data,labels) in enumerate (test_loader):\n",
    "            data= data.to(device)\n",
    "            labels= labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            # return value and index of the best class\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            true_labels_test.extend(labels.cpu().numpy())\n",
    "            predicted_labels_test.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "        test_accuracy = accuracy_score(true_labels_test, predicted_labels_test)\n",
    "        f1_test = f1_score(true_labels_test, predicted_labels_test, average='macro')\n",
    "\n",
    "\n",
    "        print(f'Accuracy = {test_accuracy:.3f}')\n",
    "        print(f'F1 Score = {f1_test:.4f}')\n",
    "\n",
    "    # Test on the whole dataset\n",
    "    with torch.no_grad():\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        for data, labels in test_all:\n",
    "            data= data.to(device)\n",
    "            labels= labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            # return value and index of the best class\n",
    "            _,predictions= torch.max(outputs,1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "            data_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "            f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "        print(f'Accuracy = {data_accuracy:.3f}')\n",
    "        print(f'F1 Score = {f1:.4f}')\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38125a83-de40-4c0a-b867-3dfba2e0e821",
   "metadata": {},
   "source": [
    "## Target: Type_of_Venom_Allergy_IGE_Venom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21d5bf0e-2d63-49a4-b129-276031e4da4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2967\n",
      "1      22\n",
      "Name: Type_of_Venom_Allergy_IGE_Venom, dtype: int64\n",
      "467 2\n",
      "TomekLinks(sampling_strategy='all')\n",
      "epoch 1/60, loss = 0.74256, train_acc = 0.5047, F1 Score_Train = 0.3370\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9920, F1 Score_test = 0.4980\u001b[0m\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 2/60, loss = 0.60843, train_acc = 0.6328, F1 Score_Train = 0.6326\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8262, F1 Score_test = 0.4743\u001b[0m\n",
      " \n",
      "TomekLinks(sampling_strategy='all')\n",
      "epoch 3/60, loss = 0.36876, train_acc = 0.5487, F1 Score_Train = 0.3606\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 4/60, loss = 0.58863, train_acc = 0.7135, F1 Score_Train = 0.7133\n",
      " \n",
      "TomekLinks(sampling_strategy='all')\n",
      "epoch 5/60, loss = 0.80822, train_acc = 0.5716, F1 Score_Train = 0.3713\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 6/60, loss = 0.55313, train_acc = 0.7535, F1 Score_Train = 0.7530\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.7406, F1 Score_test = 0.4402\u001b[0m\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 7/60, loss = 0.45897, train_acc = 0.7773, F1 Score_Train = 0.7767\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.7567, F1 Score_test = 0.4563\u001b[0m\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 8/60, loss = 0.41964, train_acc = 0.8200, F1 Score_Train = 0.8195\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8342, F1 Score_test = 0.4848\u001b[0m\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 9/60, loss = 0.34436, train_acc = 0.8645, F1 Score_Train = 0.8642\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8142, F1 Score_test = 0.4820\u001b[0m\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 10/60, loss = 0.25321, train_acc = 0.8899, F1 Score_Train = 0.8896\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8864, F1 Score_test = 0.5127\u001b[0m\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 11/60, loss = 0.17360, train_acc = 0.9335, F1 Score_Train = 0.9334\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9345, F1 Score_test = 0.5375\u001b[0m\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 12/60, loss = 0.08890, train_acc = 0.9771, F1 Score_Train = 0.9771\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9586, F1 Score_test = 0.5465\u001b[0m\n",
      " \n",
      "TomekLinks(sampling_strategy='all')\n",
      "epoch 13/60, loss = 0.37250, train_acc = 0.5828, F1 Score_Train = 0.3771\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 14/60, loss = 0.27795, train_acc = 0.8640, F1 Score_Train = 0.8639\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 15/60, loss = 0.15782, train_acc = 0.9362, F1 Score_Train = 0.9361\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 16/60, loss = 0.12947, train_acc = 0.9636, F1 Score_Train = 0.9636\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9652, F1 Score_test = 0.5578\u001b[0m\n",
      " \n",
      "TomekLinks(sampling_strategy='all')\n",
      "epoch 17/60, loss = 0.42206, train_acc = 0.6004, F1 Score_Train = 0.3844\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 18/60, loss = 0.27797, train_acc = 0.8317, F1 Score_Train = 0.8312\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 19/60, loss = 0.16525, train_acc = 0.9362, F1 Score_Train = 0.9362\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 20/60, loss = 0.15389, train_acc = 0.9629, F1 Score_Train = 0.9629\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9639, F1 Score_test = 0.5817\u001b[0m\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 21/60, loss = 0.07142, train_acc = 0.9791, F1 Score_Train = 0.9791\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9706, F1 Score_test = 0.5996\u001b[0m\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 22/60, loss = 0.05751, train_acc = 0.9897, F1 Score_Train = 0.9897\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 23/60, loss = 0.06890, train_acc = 0.9764, F1 Score_Train = 0.9764\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9840, F1 Score_test = 0.6626\u001b[0m\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 24/60, loss = 0.10258, train_acc = 0.9800, F1 Score_Train = 0.9800\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 25/60, loss = 0.06835, train_acc = 0.9861, F1 Score_Train = 0.9861\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 26/60, loss = 0.05091, train_acc = 0.9856, F1 Score_Train = 0.9856\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 27/60, loss = 0.05021, train_acc = 0.9888, F1 Score_Train = 0.9888\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 28/60, loss = 0.07156, train_acc = 0.9863, F1 Score_Train = 0.9863\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 29/60, loss = 0.09035, train_acc = 0.9825, F1 Score_Train = 0.9825\n",
      "Epoch 00029: reducing learning rate of group 0 to 7.5000e-04.\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 30/60, loss = 0.06034, train_acc = 0.9829, F1 Score_Train = 0.9829\n",
      " \n",
      "TomekLinks(sampling_strategy='all')\n",
      "epoch 31/60, loss = 0.18238, train_acc = 0.6372, F1 Score_Train = 0.3995\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 32/60, loss = 0.29539, train_acc = 0.8620, F1 Score_Train = 0.8618\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 33/60, loss = 0.07502, train_acc = 0.9688, F1 Score_Train = 0.9688\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 34/60, loss = 0.07268, train_acc = 0.9755, F1 Score_Train = 0.9755\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 35/60, loss = 0.02813, train_acc = 0.9928, F1 Score_Train = 0.9928\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9840, F1 Score_test = 0.6626\u001b[0m\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 36/60, loss = 0.02837, train_acc = 0.9930, F1 Score_Train = 0.9930\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 37/60, loss = 0.02393, train_acc = 0.9973, F1 Score_Train = 0.9973\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9880, F1 Score_test = 0.6970\u001b[0m\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 38/60, loss = 0.02061, train_acc = 0.9955, F1 Score_Train = 0.9955\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 39/60, loss = 0.01567, train_acc = 0.9982, F1 Score_Train = 0.9982\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 40/60, loss = 0.05328, train_acc = 0.9825, F1 Score_Train = 0.9825\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 41/60, loss = 0.03198, train_acc = 0.9937, F1 Score_Train = 0.9937\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 42/60, loss = 0.04000, train_acc = 0.9939, F1 Score_Train = 0.9939\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 43/60, loss = 0.02244, train_acc = 0.9928, F1 Score_Train = 0.9928\n",
      "Epoch 00043: reducing learning rate of group 0 to 5.6250e-04.\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 44/60, loss = 0.02823, train_acc = 0.9944, F1 Score_Train = 0.9944\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 45/60, loss = 0.02456, train_acc = 0.9955, F1 Score_Train = 0.9955\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 46/60, loss = 0.01729, train_acc = 0.9964, F1 Score_Train = 0.9964\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 47/60, loss = 0.01040, train_acc = 0.9987, F1 Score_Train = 0.9987\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 48/60, loss = 0.01863, train_acc = 0.9971, F1 Score_Train = 0.9971\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 49/60, loss = 0.01192, train_acc = 0.9973, F1 Score_Train = 0.9973\n",
      "Epoch 00049: reducing learning rate of group 0 to 4.2188e-04.\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 50/60, loss = 0.00921, train_acc = 0.9980, F1 Score_Train = 0.9980\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 51/60, loss = 0.01025, train_acc = 0.9996, F1 Score_Train = 0.9996\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 52/60, loss = 0.03505, train_acc = 0.9885, F1 Score_Train = 0.9885\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 53/60, loss = 0.01539, train_acc = 0.9955, F1 Score_Train = 0.9955\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 54/60, loss = 0.01640, train_acc = 0.9964, F1 Score_Train = 0.9964\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 55/60, loss = 0.01644, train_acc = 0.9969, F1 Score_Train = 0.9969\n",
      "Epoch 00055: reducing learning rate of group 0 to 3.1641e-04.\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 56/60, loss = 0.01178, train_acc = 0.9993, F1 Score_Train = 0.9993\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 57/60, loss = 0.00894, train_acc = 0.9993, F1 Score_Train = 0.9993\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 58/60, loss = 0.01940, train_acc = 0.9982, F1 Score_Train = 0.9982\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 59/60, loss = 0.00996, train_acc = 0.9996, F1 Score_Train = 0.9996\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 60/60, loss = 0.01046, train_acc = 0.9996, F1 Score_Train = 0.9996\n",
      " \n",
      "Accuracy = 0.988\n",
      "F1 Score = 0.6970\n",
      "Accuracy = 0.997\n",
      "F1 Score = 0.8950\n"
     ]
    }
   ],
   "source": [
    "model=training_by_target('Type_of_Venom_Allergy_IGE_Venom',batch_size=512,learning_rate = 1e-3,dropout_rate= 0.6, weight_decay=1e-2, factor=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01b3c190-41aa-44f1-bc85-3cad7e760057",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = Allergy_Net(input_size=467, hidden_size=2048, num_class=2,dropout_rate=0.6).to(device)\n",
    "model_to_save.load_state_dict(torch.load('best_model.pth'))\n",
    "torch.save(model_to_save.state_dict(), 'Type_of_Venom_Allergy_IGE_Venom_Pytorch_2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e7723-e981-483c-b3bb-ef26e5eb0800",
   "metadata": {},
   "source": [
    "## Target: Type_of_Venom_Allergy_ATCD_Venom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cbfae6d5-28ef-4744-b3d5-45cead6e1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_techniques = {\n",
    "    3: oversampling.RandomOverSampler(),\n",
    "    #1: oversampling.SMOTE(),\n",
    "    4: oversampling.BorderlineSMOTE(),\n",
    "    #0: oversampling.SVMSMOTE(),\n",
    "    #4: undersampling.TomekLinks(sampling_strategy='auto'),\n",
    "    #4: undersampling.TomekLinks(sampling_strategy='all'),\n",
    "    5: combination.SMOTETomek(sampling_strategy='auto'),\n",
    "    6: combination.SMOTETomek(sampling_strategy='all'),\n",
    "}\n",
    "def training_by_target(column,batch_size=512,learning_rate = 1e-3,dropout_rate= 0.6, weight_decay=1e-2, factor=0.75):\n",
    "    print(Targets[column].value_counts())\n",
    "    dataset_panda= pd.concat([encode_data,Targets[column]], axis = 1)\n",
    "    \n",
    "    # Establish the splitting and the Dataset/Loaders for the evaluation part\n",
    "    batch_size= batch_size\n",
    "    train_data, test_data = train_test_split(dataset_panda, test_size=0.15, random_state=123)\n",
    "    dataset_test=CustomDataset(test_data)\n",
    "    dataset_all=CustomDataset(dataset_panda)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "    test_all = DataLoader(dataset_all, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # define hyperparameters\n",
    "    input_size= dataset_all.num_features\n",
    "    hidden_size= 2048\n",
    "    num_class = dataset_all.num_classes\n",
    "    num_epochs=60\n",
    "    learning_rate = learning_rate\n",
    "    print(input_size, num_class)\n",
    "    \n",
    "    # Call a model and define loss and optimizer\n",
    "    model= Allergy_Net(input_size,hidden_size,num_class,dropout_rate).to(device)\n",
    "    criterion= nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=factor, patience=5, verbose=True)\n",
    "    \n",
    "    \n",
    "    # Initialization of some indicators that are used to save best model during training\n",
    "    best_f1_score = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Training loop\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        random_key = random.randint(3, 6)\n",
    "        selected_technique = sampling_techniques[random_key]\n",
    "        print(selected_technique)\n",
    "        # Apply the sampling technique\n",
    "        train_dataset = CustomDataset(train_data, transform=selected_technique)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "         # Update class weights\n",
    "        class_weights = calculate_class_weights(train_dataset)\n",
    "        criterion.weight = class_weights\n",
    "\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        model.train()\n",
    "        for i,(data,labels) in enumerate (dataloader):\n",
    "            data= data.to(device)\n",
    "            labels= labels.to(device)\n",
    "\n",
    "            #forward\n",
    "            outputs=model(data)\n",
    "            loss= criterion(outputs,labels)\n",
    "\n",
    "            #backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Calculate some metrics\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "        acc = accuracy_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "\n",
    "        print (f'epoch {epoch+1}/{num_epochs}, loss = {loss:.5f}, train_acc = {acc:.4f}, F1 Score_Train = {f1:.4f}')\n",
    "\n",
    "        # Test \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            true_labels_test = []\n",
    "            predicted_labels_test = []\n",
    "            for i,(data,labels) in enumerate (test_loader):\n",
    "                data= data.to(device)\n",
    "                labels= labels.to(device)\n",
    "\n",
    "                outputs = model(data)\n",
    "\n",
    "                # return value and index of the best class\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "                true_labels_test.extend(labels.cpu().numpy())\n",
    "                predicted_labels_test.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "            test_accuracy = accuracy_score(true_labels_test, predicted_labels_test)\n",
    "            f1_test = f1_score(true_labels_test, predicted_labels_test, average='macro')\n",
    "\n",
    "            lr_scheduler.step(f1_test + (f1*0.1))\n",
    "\n",
    "            # Check if the current model is the best one based on f1 score\n",
    "\n",
    "            if f1_test + (f1*0.5) > best_f1_score:\n",
    "                best_f1_score = f1_test+(f1*0.5) \n",
    "                best_model_state = model.state_dict()\n",
    "                torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "                print('\\033[91m'+'MODEL_SAVE')\n",
    "                print(f'Accuracy_test = {test_accuracy:.4f}, F1 Score_test = {f1_test:.4f}'+'\\033[0m')\n",
    "\n",
    "            print(' ')\n",
    "            \n",
    "    # eval_final_before saving        \n",
    "    model = Allergy_Net(input_size, hidden_size, num_class,dropout_rate).to(device)\n",
    "\n",
    "    # Load the saved model state dictionary\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Test       \n",
    "    with torch.no_grad():\n",
    "        true_labels_test = []\n",
    "        predicted_labels_test = []\n",
    "        for i,(data,labels) in enumerate (test_loader):\n",
    "            data= data.to(device)\n",
    "            labels= labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            # return value and index of the best class\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            true_labels_test.extend(labels.cpu().numpy())\n",
    "            predicted_labels_test.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "        test_accuracy = accuracy_score(true_labels_test, predicted_labels_test)\n",
    "        f1_test = f1_score(true_labels_test, predicted_labels_test, average='macro')\n",
    "\n",
    "\n",
    "        print(f'Accuracy = {test_accuracy:.3f}')\n",
    "        print(f'F1 Score = {f1_test:.4f}')\n",
    "\n",
    "    # Test on the whole dataset\n",
    "    with torch.no_grad():\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        for data, labels in test_all:\n",
    "            data= data.to(device)\n",
    "            labels= labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            # return value and index of the best class\n",
    "            _,predictions= torch.max(outputs,1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "            data_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "            f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "        print(f'Accuracy = {data_accuracy:.3f}')\n",
    "        print(f'F1 Score = {f1:.4f}')\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f2869dc2-9459-4606-a4e3-fadd9507cff2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2968\n",
      "1      21\n",
      "Name: Type_of_Venom_Allergy_ATCD_Venom, dtype: int64\n",
      "467 2\n",
      "BorderlineSMOTE()\n",
      "epoch 1/60, loss = 0.22585, train_acc = 0.8542, F1 Score_Train = 0.8534\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9332, F1 Score_test = 0.4827\u001b[0m\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 2/60, loss = 0.55738, train_acc = 0.6375, F1 Score_Train = 0.6350\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 3/60, loss = 0.50204, train_acc = 0.7315, F1 Score_Train = 0.7292\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 4/60, loss = 0.44708, train_acc = 0.7729, F1 Score_Train = 0.7690\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 5/60, loss = 0.18578, train_acc = 0.9064, F1 Score_Train = 0.9056\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9154, F1 Score_test = 0.4779\u001b[0m\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 6/60, loss = 0.53277, train_acc = 0.6852, F1 Score_Train = 0.6820\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 7/60, loss = 0.39316, train_acc = 0.8120, F1 Score_Train = 0.8102\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 8/60, loss = 0.14250, train_acc = 0.9480, F1 Score_Train = 0.9479\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9220, F1 Score_test = 0.4797\u001b[0m\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 9/60, loss = 0.36035, train_acc = 0.7869, F1 Score_Train = 0.7861\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 10/60, loss = 0.27205, train_acc = 0.8503, F1 Score_Train = 0.8495\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 11/60, loss = 0.21751, train_acc = 0.9052, F1 Score_Train = 0.9048\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 12/60, loss = 0.21652, train_acc = 0.9270, F1 Score_Train = 0.9267\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 13/60, loss = 0.05827, train_acc = 0.9891, F1 Score_Train = 0.9891\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9822, F1 Score_test = 0.4955\u001b[0m\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 14/60, loss = 0.28276, train_acc = 0.7979, F1 Score_Train = 0.7976\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 15/60, loss = 0.20670, train_acc = 0.9088, F1 Score_Train = 0.9083\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 16/60, loss = 0.19480, train_acc = 0.9286, F1 Score_Train = 0.9286\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 17/60, loss = 0.09600, train_acc = 0.9615, F1 Score_Train = 0.9615\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9310, F1 Score_test = 0.5124\u001b[0m\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 18/60, loss = 0.09150, train_acc = 0.9647, F1 Score_Train = 0.9647\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9599, F1 Score_test = 0.5397\u001b[0m\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 19/60, loss = 0.09296, train_acc = 0.9738, F1 Score_Train = 0.9738\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9822, F1 Score_test = 0.5955\u001b[0m\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 20/60, loss = 0.07131, train_acc = 0.9695, F1 Score_Train = 0.9695\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 21/60, loss = 0.02808, train_acc = 0.9956, F1 Score_Train = 0.9956\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 22/60, loss = 0.01995, train_acc = 0.9968, F1 Score_Train = 0.9968\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 23/60, loss = 0.23011, train_acc = 0.8823, F1 Score_Train = 0.8823\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 24/60, loss = 0.11114, train_acc = 0.9449, F1 Score_Train = 0.9448\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 25/60, loss = 0.07705, train_acc = 0.9704, F1 Score_Train = 0.9704\n",
      "Epoch 00025: reducing learning rate of group 0 to 4.0000e-04.\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 26/60, loss = 0.10562, train_acc = 0.9683, F1 Score_Train = 0.9683\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 27/60, loss = 0.03374, train_acc = 0.9970, F1 Score_Train = 0.9970\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 28/60, loss = 0.01017, train_acc = 0.9972, F1 Score_Train = 0.9972\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 29/60, loss = 0.01067, train_acc = 0.9966, F1 Score_Train = 0.9966\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 30/60, loss = 0.11302, train_acc = 0.8945, F1 Score_Train = 0.8944\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 31/60, loss = 0.09712, train_acc = 0.9589, F1 Score_Train = 0.9589\n",
      "Epoch 00031: reducing learning rate of group 0 to 3.2000e-04.\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 32/60, loss = 0.01959, train_acc = 0.9960, F1 Score_Train = 0.9960\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 33/60, loss = 0.14696, train_acc = 0.9022, F1 Score_Train = 0.9021\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 34/60, loss = 0.08201, train_acc = 0.9728, F1 Score_Train = 0.9728\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 35/60, loss = 0.04837, train_acc = 0.9788, F1 Score_Train = 0.9788\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 36/60, loss = 0.07116, train_acc = 0.9827, F1 Score_Train = 0.9827\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9866, F1 Score_test = 0.6216\u001b[0m\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 37/60, loss = 0.06810, train_acc = 0.9839, F1 Score_Train = 0.9839\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9866, F1 Score_test = 0.6216\u001b[0m\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 38/60, loss = 0.03344, train_acc = 0.9887, F1 Score_Train = 0.9887\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 39/60, loss = 0.03549, train_acc = 0.9901, F1 Score_Train = 0.9901\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 40/60, loss = 0.06108, train_acc = 0.9889, F1 Score_Train = 0.9889\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9866, F1 Score_test = 0.6216\u001b[0m\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 41/60, loss = 0.03282, train_acc = 0.9923, F1 Score_Train = 0.9923\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 42/60, loss = 0.02998, train_acc = 0.9911, F1 Score_Train = 0.9911\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 43/60, loss = 0.03633, train_acc = 0.9925, F1 Score_Train = 0.9925\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9866, F1 Score_test = 0.6216\u001b[0m\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 44/60, loss = 0.02039, train_acc = 0.9980, F1 Score_Train = 0.9980\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 45/60, loss = 0.09718, train_acc = 0.9204, F1 Score_Train = 0.9203\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 46/60, loss = 0.04146, train_acc = 0.9825, F1 Score_Train = 0.9825\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 47/60, loss = 0.01779, train_acc = 0.9962, F1 Score_Train = 0.9962\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 48/60, loss = 0.10736, train_acc = 0.9125, F1 Score_Train = 0.9122\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 49/60, loss = 0.06495, train_acc = 0.9758, F1 Score_Train = 0.9758\n",
      "Epoch 00049: reducing learning rate of group 0 to 2.5600e-04.\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 50/60, loss = 0.04820, train_acc = 0.9865, F1 Score_Train = 0.9865\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 51/60, loss = 0.04027, train_acc = 0.9889, F1 Score_Train = 0.9889\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 52/60, loss = 0.03419, train_acc = 0.9944, F1 Score_Train = 0.9944\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 53/60, loss = 0.01181, train_acc = 0.9942, F1 Score_Train = 0.9942\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 54/60, loss = 0.02386, train_acc = 0.9942, F1 Score_Train = 0.9942\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9889, F1 Score_test = 0.6401\u001b[0m\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 55/60, loss = 0.01483, train_acc = 0.9970, F1 Score_Train = 0.9970\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 56/60, loss = 0.01091, train_acc = 0.9960, F1 Score_Train = 0.9960\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 57/60, loss = 0.01488, train_acc = 0.9966, F1 Score_Train = 0.9966\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.9889, F1 Score_test = 0.6401\u001b[0m\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 58/60, loss = 0.01650, train_acc = 0.9960, F1 Score_Train = 0.9960\n",
      " \n",
      "SMOTETomek(sampling_strategy='all')\n",
      "epoch 59/60, loss = 0.03122, train_acc = 0.9962, F1 Score_Train = 0.9962\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 60/60, loss = 0.01775, train_acc = 0.9948, F1 Score_Train = 0.9948\n",
      " \n",
      "Accuracy = 0.989\n",
      "F1 Score = 0.6401\n",
      "Accuracy = 0.995\n",
      "F1 Score = 0.8692\n"
     ]
    }
   ],
   "source": [
    "model=training_by_target('Type_of_Venom_Allergy_ATCD_Venom',batch_size=64,learning_rate = 1e-3,dropout_rate= 0.25, weight_decay=1e-2, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b4e3ad0f-ef92-4d15-8c23-e7020ec8c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = Allergy_Net(input_size=467, hidden_size=2048, num_class=2,dropout_rate=0.6).to(device)\n",
    "model_to_save.load_state_dict(torch.load('best_model.pth'))\n",
    "torch.save(model_to_save.state_dict(), 'Type_of_Venom_Allergy_ATCD_Venom_Pytorch_2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b37fe-a510-45ae-8d25-8ebaa63e0c1f",
   "metadata": {},
   "source": [
    "## Target: Type_of_Respiratory_Allergy_IGE_Molds_Yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c35b0ff1-46a4-4d9f-8969-3e2f58313d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_techniques = {\n",
    "    0: oversampling.RandomOverSampler(),\n",
    "    1: oversampling.SMOTE(),\n",
    "    2: oversampling.BorderlineSMOTE(),\n",
    "    3: oversampling.SVMSMOTE(),\n",
    "    4: undersampling.RandomUnderSampler(),\n",
    "    5: undersampling.TomekLinks(),\n",
    "    6: undersampling.NearMiss(),\n",
    "    7: undersampling.EditedNearestNeighbours(),\n",
    "    8: combination.SMOTEENN(),\n",
    "    9: combination.SMOTETomek()\n",
    "}\n",
    "class Allergy_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,hidden_size, num_class,dropout_rate):\n",
    "\n",
    "        super(Allergy_Net,self).__init__()\n",
    "        self.linear1= nn.Linear(input_size,hidden_size)\n",
    "        self.linear2= nn.Linear(hidden_size,int(hidden_size/8))\n",
    "        self.linear3= nn.Linear(int(hidden_size/8),int(hidden_size/16))\n",
    "        self.linear4= nn.Linear(int(hidden_size/16),num_class)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(int(hidden_size/16))\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        x = torch.relu(self.linear1(inputs))\n",
    "        x= self.dropout1(x)\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x= self.dropout1(x)\n",
    "        x = torch.relu(self.linear3(x))\n",
    "        x= self.dropout1(x)\n",
    "        x= self.batchnorm1(x)\n",
    "        outputs= self.linear4(x)\n",
    "\n",
    "        # no softmax because Cross entropy Loss\n",
    "        return outputs\n",
    "    \n",
    "def training_by_target(column,batch_size=512,learning_rate = 1e-3,dropout_rate= 0.6, weight_decay=1e-2, factor=0.75):\n",
    "    print(Targets[column].value_counts())\n",
    "    dataset_panda= pd.concat([encode_data,Targets[column]], axis = 1)\n",
    "    \n",
    "    # Establish the splitting and the Dataset/Loaders for the evaluation part\n",
    "    batch_size= batch_size\n",
    "    train_data, test_data = train_test_split(dataset_panda, test_size=0.2, random_state=123)\n",
    "    dataset_test=CustomDataset(test_data)\n",
    "    dataset_all=CustomDataset(dataset_panda)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "    test_all = DataLoader(dataset_all, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # define hyperparameters\n",
    "    input_size= dataset_all.num_features\n",
    "    hidden_size= 2048\n",
    "    num_class = dataset_all.num_classes\n",
    "    num_epochs=60\n",
    "    learning_rate = learning_rate\n",
    "    print(input_size, num_class)\n",
    "    \n",
    "    # Call a model and define loss and optimizer\n",
    "    model= Allergy_Net(input_size,hidden_size,num_class,dropout_rate).to(device)\n",
    "    criterion= nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=factor, patience=5, verbose=True)\n",
    "    \n",
    "    \n",
    "    # Initialization of some indicators that are used to save best model during training\n",
    "    best_f1_score = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Training loop\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        random_key = random.randint(0, 9)\n",
    "        selected_technique = sampling_techniques[random_key]\n",
    "        print(selected_technique)\n",
    "        # Apply the sampling technique\n",
    "        train_dataset = CustomDataset(train_data, transform=selected_technique)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "         # Update class weights\n",
    "        #class_weights = calculate_class_weights(train_dataset)\n",
    "        #criterion.weight = class_weights\n",
    "\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        model.train()\n",
    "        for i,(data,labels) in enumerate (dataloader):\n",
    "            data= data.to(device)\n",
    "            labels= labels.to(device)\n",
    "\n",
    "            #forward\n",
    "            outputs=model(data)\n",
    "            loss= criterion(outputs,labels)\n",
    "\n",
    "            #backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Calculate some metrics\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "        acc = accuracy_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "\n",
    "        print (f'epoch {epoch+1}/{num_epochs}, loss = {loss:.5f}, train_acc = {acc:.4f}, F1 Score_Train = {f1:.4f}')\n",
    "\n",
    "        # Test \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            true_labels_test = []\n",
    "            predicted_labels_test = []\n",
    "            for i,(data,labels) in enumerate (test_loader):\n",
    "                data= data.to(device)\n",
    "                labels= labels.to(device)\n",
    "\n",
    "                outputs = model(data)\n",
    "\n",
    "                # return value and index of the best class\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "                true_labels_test.extend(labels.cpu().numpy())\n",
    "                predicted_labels_test.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "            test_accuracy = accuracy_score(true_labels_test, predicted_labels_test)\n",
    "            f1_test = f1_score(true_labels_test, predicted_labels_test, average='macro')\n",
    "\n",
    "            lr_scheduler.step(f1_test + (f1*0.1))\n",
    "\n",
    "            # Check if the current model is the best one based on f1 score\n",
    "\n",
    "            if f1_test + (f1*0.05) > best_f1_score:\n",
    "                best_f1_score = f1_test+(f1*0.05) \n",
    "                best_model_state = model.state_dict()\n",
    "                torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "                print('\\033[91m'+'MODEL_SAVE')\n",
    "                print(f'Accuracy_test = {test_accuracy:.4f}, F1 Score_test = {f1_test:.4f}'+'\\033[0m')\n",
    "\n",
    "            print(' ')\n",
    "            \n",
    "    # eval_final_before saving        \n",
    "    model = Allergy_Net(input_size, hidden_size, num_class,dropout_rate).to(device)\n",
    "\n",
    "    # Load the saved model state dictionary\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Test       \n",
    "    with torch.no_grad():\n",
    "        true_labels_test = []\n",
    "        predicted_labels_test = []\n",
    "        for i,(data,labels) in enumerate (test_loader):\n",
    "            data= data.to(device)\n",
    "            labels= labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            # return value and index of the best class\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            true_labels_test.extend(labels.cpu().numpy())\n",
    "            predicted_labels_test.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "        test_accuracy = accuracy_score(true_labels_test, predicted_labels_test)\n",
    "        f1_test = f1_score(true_labels_test, predicted_labels_test, average='weighted')\n",
    "\n",
    "\n",
    "        print(f'Accuracy = {test_accuracy:.3f}')\n",
    "        print(f'F1 Score = {f1_test:.4f}')\n",
    "\n",
    "    # Test on the whole dataset\n",
    "    with torch.no_grad():\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        for data, labels in test_all:\n",
    "            data= data.to(device)\n",
    "            labels= labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            # return value and index of the best class\n",
    "            _,predictions= torch.max(outputs,1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "            data_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "            f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "        print(f'Accuracy = {data_accuracy:.3f}')\n",
    "        print(f'F1 Score = {f1:.4f}')\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2e5a6dc4-f716-4b5d-98bf-5da2d12edde5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    1482\n",
      "0    1166\n",
      "1     341\n",
      "Name: Type_of_Respiratory_Allergy_IGE_Molds_Yeast, dtype: int64\n",
      "467 3\n",
      "RandomUnderSampler()\n",
      "epoch 1/60, loss = 1.03035, train_acc = 0.4242, F1 Score_Train = 0.4204\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.6622, F1 Score_test = 0.4976\u001b[0m\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 2/60, loss = 0.71245, train_acc = 0.5992, F1 Score_Train = 0.5745\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.6706, F1 Score_test = 0.5356\u001b[0m\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 3/60, loss = 0.70223, train_acc = 0.6481, F1 Score_Train = 0.6243\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.7860, F1 Score_test = 0.6298\u001b[0m\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 4/60, loss = 0.58468, train_acc = 0.7137, F1 Score_Train = 0.7068\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.7324, F1 Score_test = 0.6342\u001b[0m\n",
      " \n",
      "SMOTE()\n",
      "epoch 5/60, loss = 0.60639, train_acc = 0.7492, F1 Score_Train = 0.7467\n",
      " \n",
      "EditedNearestNeighbours()\n",
      "epoch 6/60, loss = 0.27077, train_acc = 0.8588, F1 Score_Train = 0.8115\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8395, F1 Score_test = 0.7221\u001b[0m\n",
      " \n",
      "SMOTE()\n",
      "epoch 7/60, loss = 0.47990, train_acc = 0.7794, F1 Score_Train = 0.7770\n",
      " \n",
      "TomekLinks()\n",
      "epoch 8/60, loss = 0.50095, train_acc = 0.8357, F1 Score_Train = 0.7624\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8328, F1 Score_test = 0.7449\u001b[0m\n",
      " \n",
      "TomekLinks()\n",
      "epoch 9/60, loss = 0.44890, train_acc = 0.8449, F1 Score_Train = 0.7680\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 10/60, loss = 0.34673, train_acc = 0.7938, F1 Score_Train = 0.7913\n",
      " \n",
      "NearMiss()\n",
      "epoch 11/60, loss = 0.76787, train_acc = 0.8230, F1 Score_Train = 0.8205\n",
      " \n",
      "SVMSMOTE()\n",
      "epoch 12/60, loss = 0.36387, train_acc = 0.8224, F1 Score_Train = 0.8202\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8528, F1 Score_test = 0.7808\u001b[0m\n",
      " \n",
      "NearMiss()\n",
      "epoch 13/60, loss = 0.49178, train_acc = 0.8279, F1 Score_Train = 0.8267\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 14/60, loss = 0.42188, train_acc = 0.8181, F1 Score_Train = 0.8167\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8495, F1 Score_test = 0.7856\u001b[0m\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 15/60, loss = 0.39439, train_acc = 0.8413, F1 Score_Train = 0.8407\n",
      " \n",
      "TomekLinks()\n",
      "epoch 16/60, loss = 0.45186, train_acc = 0.8629, F1 Score_Train = 0.8114\n",
      " \n",
      "SVMSMOTE()\n",
      "epoch 17/60, loss = 0.27527, train_acc = 0.8442, F1 Score_Train = 0.8431\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 18/60, loss = 0.45642, train_acc = 0.8451, F1 Score_Train = 0.8447\n",
      " \n",
      "EditedNearestNeighbours()\n",
      "epoch 19/60, loss = 0.23408, train_acc = 0.9057, F1 Score_Train = 0.8700\n",
      " \n",
      "EditedNearestNeighbours()\n",
      "epoch 20/60, loss = 0.35229, train_acc = 0.9285, F1 Score_Train = 0.8948\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8462, F1 Score_test = 0.7826\u001b[0m\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 21/60, loss = 0.63821, train_acc = 0.8339, F1 Score_Train = 0.8333\n",
      " \n",
      "SMOTEENN()\n",
      "epoch 22/60, loss = 0.26634, train_acc = 0.9063, F1 Score_Train = 0.9060\n",
      " \n",
      "SMOTE()\n",
      "epoch 23/60, loss = 0.32022, train_acc = 0.8346, F1 Score_Train = 0.8340\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8545, F1 Score_test = 0.7970\u001b[0m\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 24/60, loss = 0.29643, train_acc = 0.8676, F1 Score_Train = 0.8672\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8679, F1 Score_test = 0.8026\u001b[0m\n",
      " \n",
      "TomekLinks()\n",
      "epoch 25/60, loss = 0.50087, train_acc = 0.8761, F1 Score_Train = 0.8343\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8729, F1 Score_test = 0.8100\u001b[0m\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 26/60, loss = 0.51944, train_acc = 0.8291, F1 Score_Train = 0.8277\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8746, F1 Score_test = 0.8226\u001b[0m\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 27/60, loss = 0.32605, train_acc = 0.8585, F1 Score_Train = 0.8575\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 28/60, loss = 0.38761, train_acc = 0.8628, F1 Score_Train = 0.8617\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8729, F1 Score_test = 0.8250\u001b[0m\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 29/60, loss = 0.43830, train_acc = 0.8570, F1 Score_Train = 0.8564\n",
      " \n",
      "SMOTE()\n",
      "epoch 30/60, loss = 0.29227, train_acc = 0.8608, F1 Score_Train = 0.8602\n",
      " \n",
      "SMOTEENN()\n",
      "epoch 31/60, loss = 0.14067, train_acc = 0.9211, F1 Score_Train = 0.9210\n",
      " \n",
      "NearMiss()\n",
      "epoch 32/60, loss = 0.59346, train_acc = 0.8521, F1 Score_Train = 0.8517\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 33/60, loss = 0.46899, train_acc = 0.7685, F1 Score_Train = 0.7637\n",
      " \n",
      "EditedNearestNeighbours()\n",
      "epoch 34/60, loss = 0.25263, train_acc = 0.9081, F1 Score_Train = 0.8732\n",
      "Epoch 00034: reducing learning rate of group 0 to 6.0000e-04.\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 35/60, loss = 0.51112, train_acc = 0.8436, F1 Score_Train = 0.8422\n",
      " \n",
      "TomekLinks()\n",
      "epoch 36/60, loss = 0.39758, train_acc = 0.8779, F1 Score_Train = 0.8347\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 37/60, loss = 0.34240, train_acc = 0.8643, F1 Score_Train = 0.8639\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 38/60, loss = 0.38448, train_acc = 0.8812, F1 Score_Train = 0.8808\n",
      " \n",
      "SMOTE()\n",
      "epoch 39/60, loss = 0.29304, train_acc = 0.8896, F1 Score_Train = 0.8896\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 40/60, loss = 0.29431, train_acc = 0.9009, F1 Score_Train = 0.9009\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.8000e-04.\n",
      " \n",
      "TomekLinks()\n",
      "epoch 41/60, loss = 0.38803, train_acc = 0.8915, F1 Score_Train = 0.8644\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 42/60, loss = 0.39363, train_acc = 0.8655, F1 Score_Train = 0.8648\n",
      " \n",
      "EditedNearestNeighbours()\n",
      "epoch 43/60, loss = 0.16776, train_acc = 0.9587, F1 Score_Train = 0.9453\n",
      " \n",
      "SMOTEENN()\n",
      "epoch 44/60, loss = 0.15656, train_acc = 0.9408, F1 Score_Train = 0.9406\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 45/60, loss = 0.34613, train_acc = 0.9102, F1 Score_Train = 0.9101\n",
      " \n",
      "SVMSMOTE()\n",
      "epoch 46/60, loss = 0.26736, train_acc = 0.9210, F1 Score_Train = 0.9207\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8779, F1 Score_test = 0.8240\u001b[0m\n",
      " \n",
      "NearMiss()\n",
      "epoch 47/60, loss = 0.36294, train_acc = 0.9042, F1 Score_Train = 0.9046\n",
      " \n",
      "SVMSMOTE()\n",
      "epoch 48/60, loss = 0.32070, train_acc = 0.9227, F1 Score_Train = 0.9224\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 49/60, loss = 0.26768, train_acc = 0.9087, F1 Score_Train = 0.9085\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 50/60, loss = 0.32892, train_acc = 0.9166, F1 Score_Train = 0.9163\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 51/60, loss = 0.31967, train_acc = 0.9279, F1 Score_Train = 0.9278\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 52/60, loss = 0.31089, train_acc = 0.9204, F1 Score_Train = 0.9201\n",
      "Epoch 00052: reducing learning rate of group 0 to 5.4000e-05.\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 53/60, loss = 0.16755, train_acc = 0.9183, F1 Score_Train = 0.9180\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 54/60, loss = 0.11380, train_acc = 0.9250, F1 Score_Train = 0.9250\n",
      " \n",
      "EditedNearestNeighbours()\n",
      "epoch 55/60, loss = 0.12071, train_acc = 0.9630, F1 Score_Train = 0.9546\n",
      " \n",
      "NearMiss()\n",
      "epoch 56/60, loss = 0.32341, train_acc = 0.9091, F1 Score_Train = 0.9099\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8813, F1 Score_test = 0.8305\u001b[0m\n",
      " \n",
      "SVMSMOTE()\n",
      "epoch 57/60, loss = 0.30226, train_acc = 0.9297, F1 Score_Train = 0.9294\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 58/60, loss = 0.13038, train_acc = 0.9327, F1 Score_Train = 0.9326\n",
      " \n",
      "SMOTE()\n",
      "epoch 59/60, loss = 0.25774, train_acc = 0.9215, F1 Score_Train = 0.9216\n",
      " \n",
      "NearMiss()\n",
      "epoch 60/60, loss = 0.30950, train_acc = 0.9115, F1 Score_Train = 0.9116\n",
      " \n",
      "Accuracy = 0.881\n",
      "F1 Score = 0.8808\n",
      "Accuracy = 0.923\n",
      "F1 Score = 0.9089\n"
     ]
    }
   ],
   "source": [
    "model=training_by_target('Type_of_Respiratory_Allergy_IGE_Molds_Yeast',batch_size=128,learning_rate = 2e-3,dropout_rate= 0.3, weight_decay=1e-2, factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "882197e2-8272-4cc8-9dc2-950634edad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = Allergy_Net(input_size=467, hidden_size=2048, num_class=3,dropout_rate=0.6).to(device)\n",
    "model_to_save.load_state_dict(torch.load('best_model.pth'))\n",
    "torch.save(model_to_save.state_dict(), 'Type_of_Respiratory_Allergy_IGE_Molds_Yeast_Pytorch_2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878e290-16a8-4c72-9853-9780a0f80ca3",
   "metadata": {},
   "source": [
    "## Target: Type_of_Food_Allergy_Tree_Nuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ec0ebc6c-4495-409b-90c6-27cbbd3160bf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1463\n",
      "2    1382\n",
      "1     144\n",
      "Name: Type_of_Food_Allergy_Tree_Nuts, dtype: int64\n",
      "467 3\n",
      "RandomUnderSampler()\n",
      "epoch 1/60, loss = 0.95284, train_acc = 0.5510, F1 Score_Train = 0.5383\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.5535, F1 Score_test = 0.3594\u001b[0m\n",
      " \n",
      "TomekLinks()\n",
      "epoch 2/60, loss = 0.68138, train_acc = 0.7216, F1 Score_Train = 0.5508\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8344, F1 Score_test = 0.5642\u001b[0m\n",
      " \n",
      "NearMiss()\n",
      "epoch 3/60, loss = 0.70053, train_acc = 0.6364, F1 Score_Train = 0.5743\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8161, F1 Score_test = 0.5723\u001b[0m\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 4/60, loss = 1.07907, train_acc = 0.5758, F1 Score_Train = 0.5423\n",
      " \n",
      "SMOTEENN()\n",
      "epoch 5/60, loss = 0.18240, train_acc = 0.8090, F1 Score_Train = 0.7476\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 6/60, loss = 0.74215, train_acc = 0.6970, F1 Score_Train = 0.6526\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 7/60, loss = 0.49269, train_acc = 0.7761, F1 Score_Train = 0.7671\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.7893, F1 Score_test = 0.6555\u001b[0m\n",
      " \n",
      "TomekLinks()\n",
      "epoch 8/60, loss = 0.40640, train_acc = 0.8235, F1 Score_Train = 0.6988\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8462, F1 Score_test = 0.7434\u001b[0m\n",
      " \n",
      "SMOTEENN()\n",
      "epoch 9/60, loss = 0.26183, train_acc = 0.8579, F1 Score_Train = 0.8428\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 10/60, loss = 0.51960, train_acc = 0.8409, F1 Score_Train = 0.8352\n",
      " \n",
      "SMOTE()\n",
      "epoch 11/60, loss = 0.38105, train_acc = 0.8433, F1 Score_Train = 0.8409\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 12/60, loss = 0.39840, train_acc = 0.8099, F1 Score_Train = 0.8069\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 13/60, loss = 0.14441, train_acc = 0.8568, F1 Score_Train = 0.8543\n",
      " \n",
      "NearMiss()\n",
      "epoch 14/60, loss = 0.37269, train_acc = 0.8430, F1 Score_Train = 0.8460\n",
      "Epoch 00014: reducing learning rate of group 0 to 2.0000e-04.\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 15/60, loss = 0.50664, train_acc = 0.8671, F1 Score_Train = 0.8637\n",
      " \n",
      "NearMiss()\n",
      "epoch 16/60, loss = 0.33620, train_acc = 0.8457, F1 Score_Train = 0.8487\n",
      " \n",
      "SMOTE()\n",
      "epoch 17/60, loss = 0.26129, train_acc = 0.8776, F1 Score_Train = 0.8755\n",
      " \n",
      "SMOTE()\n",
      "epoch 18/60, loss = 0.16826, train_acc = 0.8875, F1 Score_Train = 0.8862\n",
      " \n",
      "EditedNearestNeighbours()\n",
      "epoch 19/60, loss = 0.30133, train_acc = 0.9505, F1 Score_Train = 0.9122\n",
      " \n",
      "TomekLinks()\n",
      "epoch 20/60, loss = 0.35133, train_acc = 0.8834, F1 Score_Train = 0.8478\n",
      " \n",
      "SVMSMOTE()\n",
      "epoch 21/60, loss = 0.19355, train_acc = 0.8693, F1 Score_Train = 0.8693\n",
      " \n",
      "SVMSMOTE()\n",
      "epoch 22/60, loss = 0.30473, train_acc = 0.8988, F1 Score_Train = 0.8979\n",
      " \n",
      "SMOTE()\n",
      "epoch 23/60, loss = 0.48485, train_acc = 0.8986, F1 Score_Train = 0.8974\n",
      " \n",
      "TomekLinks()\n",
      "epoch 24/60, loss = 0.24589, train_acc = 0.8762, F1 Score_Train = 0.8255\n",
      " \n",
      "NearMiss()\n",
      "epoch 25/60, loss = 0.44407, train_acc = 0.7438, F1 Score_Train = 0.7293\n",
      " \n",
      "NearMiss()\n",
      "epoch 26/60, loss = 0.51132, train_acc = 0.8072, F1 Score_Train = 0.8045\n",
      " \n",
      "TomekLinks()\n",
      "epoch 27/60, loss = 0.38746, train_acc = 0.8744, F1 Score_Train = 0.8325\n",
      " \n",
      "EditedNearestNeighbours()\n",
      "epoch 28/60, loss = 0.19489, train_acc = 0.9630, F1 Score_Train = 0.9316\n",
      " \n",
      "SMOTE()\n",
      "epoch 29/60, loss = 0.34264, train_acc = 0.8814, F1 Score_Train = 0.8804\n",
      "Epoch 00029: reducing learning rate of group 0 to 4.0000e-05.\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 30/60, loss = 0.18593, train_acc = 0.9201, F1 Score_Train = 0.9192\n",
      " \n",
      "TomekLinks()\n",
      "epoch 31/60, loss = 0.20965, train_acc = 0.8727, F1 Score_Train = 0.8251\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 32/60, loss = 0.51927, train_acc = 0.7906, F1 Score_Train = 0.7884\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 33/60, loss = 0.32174, train_acc = 0.8938, F1 Score_Train = 0.8931\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8545, F1 Score_test = 0.7514\u001b[0m\n",
      " \n",
      "NearMiss()\n",
      "epoch 34/60, loss = 0.34284, train_acc = 0.8595, F1 Score_Train = 0.8607\n",
      " \n",
      "SMOTEENN()\n",
      "epoch 35/60, loss = 0.21623, train_acc = 0.9609, F1 Score_Train = 0.9541\n",
      " \n",
      "SVMSMOTE()\n",
      "epoch 36/60, loss = 0.16765, train_acc = 0.9044, F1 Score_Train = 0.9030\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 37/60, loss = 0.30229, train_acc = 0.9190, F1 Score_Train = 0.9177\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 38/60, loss = 0.46815, train_acc = 0.9096, F1 Score_Train = 0.9088\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 39/60, loss = 0.17855, train_acc = 0.9118, F1 Score_Train = 0.9116\n",
      "Epoch 00039: reducing learning rate of group 0 to 8.0000e-06.\n",
      " \n",
      "BorderlineSMOTE()\n",
      "epoch 40/60, loss = 0.23981, train_acc = 0.9071, F1 Score_Train = 0.9061\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 41/60, loss = 0.15171, train_acc = 0.9265, F1 Score_Train = 0.9250\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 42/60, loss = 0.31716, train_acc = 0.9228, F1 Score_Train = 0.9214\n",
      " \n",
      "EditedNearestNeighbours()\n",
      "epoch 43/60, loss = 0.13487, train_acc = 0.9540, F1 Score_Train = 0.9225\n",
      " \n",
      "SVMSMOTE()\n",
      "epoch 44/60, loss = 0.34604, train_acc = 0.9074, F1 Score_Train = 0.9071\n",
      " \n",
      "TomekLinks()\n",
      "epoch 45/60, loss = 0.46580, train_acc = 0.8843, F1 Score_Train = 0.8480\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.6000e-06.\n",
      " \n",
      "SMOTEENN()\n",
      "epoch 46/60, loss = 0.41795, train_acc = 0.8898, F1 Score_Train = 0.8817\n",
      " \n",
      "SMOTEENN()\n",
      "epoch 47/60, loss = 0.12372, train_acc = 0.9391, F1 Score_Train = 0.9312\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 48/60, loss = 0.31063, train_acc = 0.9212, F1 Score_Train = 0.9200\n",
      " \n",
      "SMOTEENN()\n",
      "epoch 49/60, loss = 0.26318, train_acc = 0.9597, F1 Score_Train = 0.9528\n",
      "\u001b[91mMODEL_SAVE\n",
      "Accuracy_test = 0.8579, F1 Score_test = 0.7678\u001b[0m\n",
      " \n",
      "EditedNearestNeighbours()\n",
      "epoch 50/60, loss = 0.15812, train_acc = 0.9575, F1 Score_Train = 0.9280\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 51/60, loss = 0.29980, train_acc = 0.9146, F1 Score_Train = 0.9139\n",
      " \n",
      "SVMSMOTE()\n",
      "epoch 52/60, loss = 0.17812, train_acc = 0.9116, F1 Score_Train = 0.9107\n",
      " \n",
      "NearMiss()\n",
      "epoch 53/60, loss = 0.38782, train_acc = 0.8788, F1 Score_Train = 0.8798\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 54/60, loss = 0.15501, train_acc = 0.9036, F1 Score_Train = 0.9037\n",
      " \n",
      "EditedNearestNeighbours()\n",
      "epoch 55/60, loss = 0.19102, train_acc = 0.9582, F1 Score_Train = 0.9270\n",
      "Epoch 00055: reducing learning rate of group 0 to 3.2000e-07.\n",
      " \n",
      "SMOTETomek()\n",
      "epoch 56/60, loss = 0.25943, train_acc = 0.9223, F1 Score_Train = 0.9211\n",
      " \n",
      "RandomUnderSampler()\n",
      "epoch 57/60, loss = 0.38429, train_acc = 0.9063, F1 Score_Train = 0.9065\n",
      " \n",
      "SMOTEENN()\n",
      "epoch 58/60, loss = 0.18957, train_acc = 0.9593, F1 Score_Train = 0.9528\n",
      " \n",
      "RandomOverSampler()\n",
      "epoch 59/60, loss = 0.37317, train_acc = 0.9002, F1 Score_Train = 0.8996\n",
      " \n",
      "TomekLinks()\n",
      "epoch 60/60, loss = 0.18101, train_acc = 0.8762, F1 Score_Train = 0.8305\n",
      " \n",
      "Accuracy = 0.858\n",
      "F1 Score = 0.8556\n",
      "Accuracy = 0.887\n",
      "F1 Score = 0.8716\n"
     ]
    }
   ],
   "source": [
    "model=training_by_target('Type_of_Food_Allergy_Tree_Nuts',batch_size=64,learning_rate = 1e-3,dropout_rate= 0.2, weight_decay=1e-3, factor=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14723fb-410b-4986-ac8a-44b1c53c7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = Allergy_Net(input_size=467, hidden_size=2048, num_class=3,dropout_rate=0.6).to(device)\n",
    "model_to_save.load_state_dict(torch.load('best_model.pth'))\n",
    "torch.save(model_to_save.state_dict(), 'Type_of_Food_Allergy_Tree_Nuts_Pytorch_2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
